<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>word2vec- Hierarchical Softmax | SunxiaonanBlog</title>
<meta name="description" content="经验是智慧之父，记忆是智慧之母">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="shortcut icon" href="https://Sunxiaonan0672.github.io/favicon.ico?v=1568944414851">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://unpkg.com/papercss@1.6.1/dist/paper.min.css" />
<link rel="stylesheet" href="https://Sunxiaonan0672.github.io/styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  

<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />


  </head>
  <body>
  
    <nav class="navbar border fixed split-nav">
  <div class="nav-brand">
    <h3><a href="https://Sunxiaonan0672.github.io">SunxiaonanBlog</a></h3>
  </div>
  <div class="collapsible">
    <input id="collapsible1" type="checkbox" name="collapsible1">
    <button>
      <label for="collapsible1">
        <div class="bar1"></div>
        <div class="bar2"></div>
        <div class="bar3"></div>
      </label>
    </button>
    <div class="collapsible-body">
      <ul class="inline">
        
          <li>
            
              <a href="/" class="menu">
                首页
              </a>
            
          </li>
        
          <li>
            
              <a href="/archives" class="menu">
                归档
              </a>
            
          </li>
        
          <li>
            
              <a href="/tags" class="menu">
                标签
              </a>
            
          </li>
        
          <li>
            
              <a href="/post/about" class="menu">
                关于
              </a>
            
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div id="top" class="row site">
      <div class="sm-12 md-8 col">
        <div class="paper">
          <article class="article">
            <h1>word2vec- Hierarchical Softmax</h1>
            <p class="article-meta">
              2019-09-19
              
            </p>
            
            <div class="post-content">
              <p>word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。</p>
<p><strong>1.词向量基础</strong></p>
<p>1.1-of-N representation或者one hot representation
词向量表达在很久以前就出现了，最早词向量冗长，它使用是词向量维度大小 为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。</p>
<p>比如我们有下面的6个词组成的词汇表 [&quot;king&quot;,&quot;woman&quot;,&quot;queen&quot;,&quot;man&quot;,&quot;child&quot;,&quot;kid&quot;]，  词&quot;Queen&quot;的序号为3， 那么它的词向量就是(0,0,1,0,0,0)。同样的道理，词&quot;Woman&quot;的词向量就是(0,1,0,0,0,0)。</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568864784466.png" alt=""></p>
<p>虽然One hot representation表示词向量非常简单，但是存在最大的问题是由于词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。而且只有对应词的位置是1，其余都是0</p>
<ol start="2">
<li>Distributed representation</li>
</ol>
<p>为了解决one-hot 问题，引入了Distributed representation，它的思路是通过训练，将每个词都映射到一个较短的词向量上来，所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。</p>
<p>King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568871786990.png" alt=""></p>
<p>只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。</p>
<p><strong>2.CBOW与Skip-Gram用于神经网络语言模型</strong></p>
<p>在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。</p>
<p>关于数据的输入和输出，一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。</p>
<p>1）CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是&quot;Learning&quot;，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568873991915.png" alt=""></p>
<p>这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。</p>
<p>Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为4， 特定的这个词&quot;Learning&quot;是我们的输入，而这8个上下文词是我们的输出。</p>
<p>这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前8的8个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某1个词对应的最可能的8个上下文词时，我们可以通过一次DNN前向传播算法得到概率大小排前8的softmax概率对应的神经元所对应的词即可。</p>
<p>以上就是神经网络语言模型中如何用CBOW与Skip-Gram来训练模型与得到词向量的大概过程。但是这和word2vec中用CBOW与Skip-Gram来训练模型与得到词向量的过程有很多的不同。</p>
<p>**DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，这意味着我们DNN的输出层需要进行softmax计算各个词的输出概率的的计算量很大，因此word2vec并没有采用神经网络模型。有没有简化一点点的方法呢？
**</p>
<p><strong>3.word2vec基础之霍夫曼树</strong></p>
<p>word2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。</p>
<p><a href="https://www.cnblogs.com/pinard/p/7160330.html">霍夫曼树</a></p>
<p>那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短</p>
<p><strong>4.基于Hierarchical Softmax</strong></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568876484270.png" alt=""></p>
<p>对于CBOW 和Skip-gram两个模型，word2vec给出了两个框架，分别基于Hierarchical Softmax 和Negative Sampling</p>
<p>本节介绍基于Hierarchical Softmax的CBOW 和Skip-gram的模型</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568944301012.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568944411497.png" alt=""></p>
<p>CBOW 模型的优化目标函数：</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568876979618.png" alt=""></p>
<p>Skip-gram模型的优化的目标函数：</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568877047258.png" alt=""></p>
<p>4.1 CBOW 模型</p>
<p>4.1.1 网络结构</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568902240357.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568902461847.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568902539402.png" alt=""></p>
<p>4.1.2 梯度计算</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568902867228.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568902913104.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903021858.png" alt=""></p>
<p>被分为正类的概率</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903093841.png" alt=""></p>
<p>被分为负类的概率</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903159349.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903263049.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903328172.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903398727.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903457598.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903515838.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903611813.png" alt=""></p>
<p>4.2 skip-gram模型</p>
<p>4.2.1 网络结构</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903707010.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903743785.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568903781886.png" alt=""></p>
<p>4.2.2 梯度计算</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568904427956.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568904484323.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568904535801.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568904572677.png" alt=""></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1568904617117.png" alt=""></p>
<p>4.3 基于Negative Sampling 的模型</p>
<p>Negative Sampling  是Tomas Mikolov 等人在2013年在“Distributed Representation of words and Phrases and their Compositionality”一文中提出，主要目的是提高训练速度并改善所得词向量的质量，与Hierarchical Softmax 相比，不再使用哈夫曼树，而是利用随机的负采样技术。</p>
<p><strong>引用</strong></p>
<p>1.<a href="https://www.cnblogs.com/peghoty/p/3857839.html">word2vec 数学原理</a></p>
<p>2.<a href="https://www.cnblogs.com/pinard/p/7243513.html"></a></p>

            </div>
          </article>
        </div>
        <div class="paper" data-aos="fade-in">
          
            <div class="next-post">
              <div class="next">
                下一篇
              </div>
              <a href="https://Sunxiaonan0672.github.io/post/paperrobot">
                <h3 class="post-title">
                  PaperRobot
                </h3>
              </a>
            </div>
          
        </div>
        
          
            <div class="paper" data-aos="fade-in">
              <div id="gitalk-container"></div>
            </div>
          

          
        
      </div>

      <div class="sm-12 md-4 col sidebar">
  <div class="paper info-container">
    <img src="https://Sunxiaonan0672.github.io/images/avatar.png?v=1568944414851" class="no-responsive avatar">
    <div class="text-muted">经验是智慧之父，记忆是智慧之母</div>
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
  </div>
  <div class="paper">
    <div class="sidebar-title">
      最新文章
    </div>
    <div class="row">
      <ul>
        
          
            <li>
              <a href="https://Sunxiaonan0672.github.io/post/word2vec-hierarchical-softmax">word2vec- Hierarchical Softmax</a>
            </li>
          
        
          
            <li>
              <a href="https://Sunxiaonan0672.github.io/post/paperrobot">PaperRobot</a>
            </li>
          
        
          
            <li>
              <a href="https://Sunxiaonan0672.github.io/post/recommendation">Recommendation</a>
            </li>
          
        
          
            <li>
              <a href="https://Sunxiaonan0672.github.io/post/RNN-based-on-gates">基于门控的RNN</a>
            </li>
          
        
          
            <li>
              <a href="https://Sunxiaonan0672.github.io/post/Methods">解决方法-梯度爆炸或梯度消失</a>
            </li>
          
        
          
            <li>
              <a href="https://Sunxiaonan0672.github.io/post/chang-qi-yi-lai-wen-ti">长期依赖问题</a>
            </li>
          
        
          
            <li>
              <a href="https://Sunxiaonan0672.github.io/post/RNN-参数学习">循环神经网络-RNN之参数学习</a>
            </li>
          
        
          
            <li>
              <a href="https://Sunxiaonan0672.github.io/post/RNN-Introduction">循环神经网络-RNN之简介</a>
            </li>
          
        
          
            <li>
              <a href="https://Sunxiaonan0672.github.io/post/hello-gridea">Hello Gridea</a>
            </li>
          
        
      </ul>
    </div>
  </div>
  <div class="paper">
    <div class="sidebar-title">
      标签列表
    </div>
    <div class="row">
      
        <a href="https://Sunxiaonan0672.github.io/tag/xY4B89Egk" class="badge success">
          paper
        </a>
      
        <a href="https://Sunxiaonan0672.github.io/tag/IwRrglD_0" class="badge ">
          深度学习
        </a>
      
        <a href="https://Sunxiaonan0672.github.io/tag/gridea" class="badge success">
          Gridea
        </a>
      
    </div>
  </div>
  <div class="paper">
    Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  </div>
</div>


    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>

<script type="application/javascript">

AOS.init();

hljs.initHighlightingOnLoad()

</script>


  
  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: '8eff303ed9bcf9f4ab84',
        clientSecret: 'f7db90d0f1726df30e9d7a67e8c9d39af5c2f9bc',
        repo: 'Sunxiaonan0672.github.io',
        owner: 'Sunxiaonan0672',
        admin: ['Sunxiaonan0672'],
        id: location.pathname,      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
