<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://Sunxiaonan0672.github.io</id>
    <title>SunxiaonanBlog</title>
    <updated>2019-11-29T06:38:33.678Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://Sunxiaonan0672.github.io"/>
    <link rel="self" href="https://Sunxiaonan0672.github.io/atom.xml"/>
    <subtitle>经验是智慧之父，记忆是智慧之母</subtitle>
    <logo>https://Sunxiaonan0672.github.io/images/avatar.png</logo>
    <icon>https://Sunxiaonan0672.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, SunxiaonanBlog</rights>
    <entry>
        <title type="html"><![CDATA[ELMO-Deep contextualized word representations]]></title>
        <id>https://Sunxiaonan0672.github.io/post/elmo-deep-contextualized-word-representations</id>
        <link href="https://Sunxiaonan0672.github.io/post/elmo-deep-contextualized-word-representations">
        </link>
        <updated>2019-11-28T10:21:20.000Z</updated>
        <content type="html"><![CDATA[<p><strong>1.概述</strong><br>
文章提出了一种word representations（ELMO）</p>
<p>好的词向量模型不仅能够表达词的复杂特性，比如语义和语法的特性，也要能够刻画这些特性在不同语言上下文中的变化</p>
<p>传统的词向量模型当中，学到的向量表示更多的是一种上下文独立的表示。过去一般会使用 subword information 来增强词向量表示，或者是为每个word sense 学习一个向量表示。这篇文章中的方法既有使用Character CNN来捕获 subword information，也有学习到 multi sense information。</p>
<p>文章中通过学习一个 deep bidirectional language model (biLM)，将词向量表示为 biLM 的内,部状态的函数（也不仅仅是简单的取最后一层的表示）。从而，学到的每个 word representation 不再仅仅是一个 word 的函数，而是整个 sentence 的函数。</p>
<p><strong>2.模型结构</strong></p>
<p>1）Bidirectional language models</p>
<p>给定N 个tokens 的序列 （t1,t2,..tN)</p>
<p>a forward language model computes the probability of the sequence by modeling the probability of toten tk given the history(t1,t2,...tN)</p>
<figure data-type="image" tabindex="1"><img src="https://Sunxiaonan0672.github.io/post-images/1574994688749.png" alt=""></figure>
<p>A backward LM is similar to a forward LM, except it runs over the sequence in reverse, predict- ing the previous token given the future context:</p>
<figure data-type="image" tabindex="2"><img src="https://Sunxiaonan0672.github.io/post-images/1574995054093.png" alt=""></figure>
<p><strong>BiLM 是要计算前向和后项联合极大对数似然估计</strong></p>
<figure data-type="image" tabindex="3"><img src="https://Sunxiaonan0672.github.io/post-images/1574996278300.png" alt=""></figure>
<p>2）ELMO<br>
ELMO是一个BiLM中间层表示的任务特定组合<br>
对于每个 token，L 层的 biLM 会得到 2L+1 个向量表示</p>
<figure data-type="image" tabindex="4"><img src="https://Sunxiaonan0672.github.io/post-images/1574999566885.png" alt=""></figure>
<p>对于不同特定任务，compute a task specific weighting of all biLM layers:</p>
<figure data-type="image" tabindex="5"><img src="https://Sunxiaonan0672.github.io/post-images/1574999745720.png" alt=""></figure>
<figure data-type="image" tabindex="6"><img src="https://Sunxiaonan0672.github.io/post-images/1574999795341.png" alt=""></figure>
<p><strong>3.Using biLMs for supervised NLP tasks</strong></p>
<p>假设现在已有一个 pre-trained 的 biLM，和一个任务特定的监督模型，可以很方便的利用 biLM 来提升监督模型的性能。<br>
比如，现在的监督任务模型使用通常的 word embedding，x_emb作为输入，经过几层 RNN，每个词得到一个相应的输出 h_rnn,</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1575000122298.png" alt=""><br>
<img src="https://Sunxiaonan0672.github.io/post-images/1575007116040.png" alt=""></p>
<pre><code>                        [h_k,ELMO_k]
</code></pre>
<figure data-type="image" tabindex="7"><img src="https://Sunxiaonan0672.github.io/post-images/1575007257783.png" alt=""></figure>
<p>ELMO模型中适当的加入dropout或者将ELMO模型中加入regularize，会更好。</p>
<p><strong>4.Pre-trained biLM architecture</strong></p>
<p>本文使用的 biLM 中<br>
首先对每个 word 中的 character 做 convolution 得到 word 的表示，具体的使用的 conv 共有 2048 filters， 后面接两个highway layer，再做线性投射到 512 dim，后面接 2 层 lstm. 最后为每个 word 提供了 3 层的表示<br>
而传统的 word embedding 的方式只提供了 1 层的表示。</p>
<p>在 biLM 预训练完毕之后，就可以为特定的任务计算向量表示了。<br>
文章中提到在有些 case 当中，用domain specific 的数据对 biLM 做 fine tuning 会带来 perplexity 的显著下降和下游任务性能的提升。</p>
<p><strong>5.参考</strong></p>
<p>1）<strong>ELMo: Deep contextualized word representations 阅读笔记</strong><br>
（https://www.tvect.cn/archives/55）</p>
<p>2）<strong>ELMo最好用词向量Deep Contextualized Word Representations</strong><br>
（https://zhuanlan.zhihu.com/p/38254332）<br>
3）<strong>word representations</strong><br>
（https://note.youdao.com/ynoteshare1/index.html?id=528a1e74487d642ed5b46b4e83ca99e2&amp;type=note）</p>
<p>6.作者寄语</p>
<p>来自一直永不言弃的小🦐的努力⛽️</p>
<p>​</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[NLP介绍]]></title>
        <id>https://Sunxiaonan0672.github.io/post/nlp-jie-shao</id>
        <link href="https://Sunxiaonan0672.github.io/post/nlp-jie-shao">
        </link>
        <updated>2019-11-26T03:01:19.000Z</updated>
        <content type="html"><![CDATA[<p><strong>1.什么是自然语言处理（NLP）</strong><br>
自然语言 约等于 人类语言<br>
而区别于人工语言<br>
自然语言处理包括语音识别，自然语言理解，自然语言生成，人机交互以及涉及到的中间阶段<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1574739216239.png" alt=""></p>
<p><strong>2.NLP的发展</strong><br>
<img src="https://Sunxiaonan0672.github.io/post-images/1574747428035.png" alt=""></p>
<p><strong>3.NLP分类</strong><br>
<img src="https://Sunxiaonan0672.github.io/post-images/1574747480593.png" alt=""><br>
3.1 典型的任务<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1574747677942.png" alt=""></p>
<p><strong>4.NLP的技术路线</strong><br>
<img src="https://Sunxiaonan0672.github.io/post-images/1574747802576.png" alt=""></p>
<p><strong>5.引用</strong><br>
1.面向自然语言处理的深度学习基础   邱锡鹏<br>
2.nlp学习之路-https://zhuanlan.zhihu.com/p/56802149</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[word2vec- Hierarchical Softmax]]></title>
        <id>https://Sunxiaonan0672.github.io/post/word2vec-hierarchical-softmax</id>
        <link href="https://Sunxiaonan0672.github.io/post/word2vec-hierarchical-softmax">
        </link>
        <updated>2019-09-19T00:55:31.000Z</updated>
        <content type="html"><![CDATA[<p>word2vec是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。</p>
<p><strong>1.词向量基础</strong></p>
<p>1.1-of-N representation或者one hot representation<br>
词向量表达在很久以前就出现了，最早词向量冗长，它使用是词向量维度大小 为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。</p>
<p>比如我们有下面的6个词组成的词汇表 [&quot;king&quot;,&quot;woman&quot;,&quot;queen&quot;,&quot;man&quot;,&quot;child&quot;,&quot;kid&quot;]，  词&quot;Queen&quot;的序号为3， 那么它的词向量就是(0,0,1,0,0,0)。同样的道理，词&quot;Woman&quot;的词向量就是(0,1,0,0,0,0)。</p>
<figure data-type="image" tabindex="1"><img src="https://Sunxiaonan0672.github.io/post-images/1568864784466.png" alt=""></figure>
<p>虽然One hot representation表示词向量非常简单，但是存在最大的问题是由于词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。而且只有对应词的位置是1，其余都是0</p>
<ol start="2">
<li>Distributed representation</li>
</ol>
<p>为了解决one-hot 问题，引入了Distributed representation，它的思路是通过训练，将每个词都映射到一个较短的词向量上来，所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。</p>
<p>King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。</p>
<figure data-type="image" tabindex="2"><img src="https://Sunxiaonan0672.github.io/post-images/1568871786990.png" alt=""></figure>
<p>只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。</p>
<p><strong>2.CBOW与Skip-Gram用于神经网络语言模型</strong></p>
<p>在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。</p>
<p>关于数据的输入和输出，一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。</p>
<p>1）CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是&quot;Learning&quot;，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。</p>
<figure data-type="image" tabindex="3"><img src="https://Sunxiaonan0672.github.io/post-images/1568873991915.png" alt=""></figure>
<p>这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。</p>
<p>Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为4， 特定的这个词&quot;Learning&quot;是我们的输入，而这8个上下文词是我们的输出。</p>
<p>这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前8的8个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某1个词对应的最可能的8个上下文词时，我们可以通过一次DNN前向传播算法得到概率大小排前8的softmax概率对应的神经元所对应的词即可。</p>
<p>以上就是神经网络语言模型中如何用CBOW与Skip-Gram来训练模型与得到词向量的大概过程。但是这和word2vec中用CBOW与Skip-Gram来训练模型与得到词向量的过程有很多的不同。</p>
<p>**DNN模型的这个处理过程非常耗时。我们的词汇表一般在百万级别以上，这意味着我们DNN的输出层需要进行softmax计算各个词的输出概率的的计算量很大，因此word2vec并没有采用神经网络模型。有没有简化一点点的方法呢？<br>
**</p>
<p><strong>3.word2vec基础之霍夫曼树</strong></p>
<p>word2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。</p>
<p><a href="https://www.cnblogs.com/pinard/p/7160330.html">霍夫曼树</a></p>
<p>那么霍夫曼树有什么好处呢？一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短</p>
<p><strong>4.基于Hierarchical Softmax</strong></p>
<figure data-type="image" tabindex="4"><img src="https://Sunxiaonan0672.github.io/post-images/1568876484270.png" alt=""></figure>
<p>对于CBOW 和Skip-gram两个模型，word2vec给出了两个框架，分别基于Hierarchical Softmax 和Negative Sampling</p>
<p>本节介绍基于Hierarchical Softmax的CBOW 和Skip-gram的模型</p>
<figure data-type="image" tabindex="5"><img src="https://Sunxiaonan0672.github.io/post-images/1568944301012.png" alt=""></figure>
<figure data-type="image" tabindex="6"><img src="https://Sunxiaonan0672.github.io/post-images/1568944411497.png" alt=""></figure>
<p>CBOW 模型的优化目标函数：</p>
<figure data-type="image" tabindex="7"><img src="https://Sunxiaonan0672.github.io/post-images/1568876979618.png" alt=""></figure>
<p>Skip-gram模型的优化的目标函数：</p>
<figure data-type="image" tabindex="8"><img src="https://Sunxiaonan0672.github.io/post-images/1568877047258.png" alt=""></figure>
<p>4.1 CBOW 模型</p>
<p>4.1.1 网络结构</p>
<figure data-type="image" tabindex="9"><img src="https://Sunxiaonan0672.github.io/post-images/1568902240357.png" alt=""></figure>
<figure data-type="image" tabindex="10"><img src="https://Sunxiaonan0672.github.io/post-images/1568902461847.png" alt=""></figure>
<figure data-type="image" tabindex="11"><img src="https://Sunxiaonan0672.github.io/post-images/1568902539402.png" alt=""></figure>
<p>4.1.2 梯度计算</p>
<figure data-type="image" tabindex="12"><img src="https://Sunxiaonan0672.github.io/post-images/1568902867228.png" alt=""></figure>
<figure data-type="image" tabindex="13"><img src="https://Sunxiaonan0672.github.io/post-images/1568902913104.png" alt=""></figure>
<figure data-type="image" tabindex="14"><img src="https://Sunxiaonan0672.github.io/post-images/1568903021858.png" alt=""></figure>
<p>被分为正类的概率</p>
<figure data-type="image" tabindex="15"><img src="https://Sunxiaonan0672.github.io/post-images/1568903093841.png" alt=""></figure>
<p>被分为负类的概率</p>
<figure data-type="image" tabindex="16"><img src="https://Sunxiaonan0672.github.io/post-images/1568903159349.png" alt=""></figure>
<figure data-type="image" tabindex="17"><img src="https://Sunxiaonan0672.github.io/post-images/1568903263049.png" alt=""></figure>
<figure data-type="image" tabindex="18"><img src="https://Sunxiaonan0672.github.io/post-images/1568903328172.png" alt=""></figure>
<figure data-type="image" tabindex="19"><img src="https://Sunxiaonan0672.github.io/post-images/1568903398727.png" alt=""></figure>
<figure data-type="image" tabindex="20"><img src="https://Sunxiaonan0672.github.io/post-images/1568903457598.png" alt=""></figure>
<figure data-type="image" tabindex="21"><img src="https://Sunxiaonan0672.github.io/post-images/1568903515838.png" alt=""></figure>
<figure data-type="image" tabindex="22"><img src="https://Sunxiaonan0672.github.io/post-images/1568903611813.png" alt=""></figure>
<p>4.2 skip-gram模型</p>
<p>4.2.1 网络结构</p>
<figure data-type="image" tabindex="23"><img src="https://Sunxiaonan0672.github.io/post-images/1568903707010.png" alt=""></figure>
<figure data-type="image" tabindex="24"><img src="https://Sunxiaonan0672.github.io/post-images/1568903743785.png" alt=""></figure>
<figure data-type="image" tabindex="25"><img src="https://Sunxiaonan0672.github.io/post-images/1568903781886.png" alt=""></figure>
<p>4.2.2 梯度计算</p>
<figure data-type="image" tabindex="26"><img src="https://Sunxiaonan0672.github.io/post-images/1568904427956.png" alt=""></figure>
<figure data-type="image" tabindex="27"><img src="https://Sunxiaonan0672.github.io/post-images/1568904484323.png" alt=""></figure>
<figure data-type="image" tabindex="28"><img src="https://Sunxiaonan0672.github.io/post-images/1568904535801.png" alt=""></figure>
<figure data-type="image" tabindex="29"><img src="https://Sunxiaonan0672.github.io/post-images/1568904572677.png" alt=""></figure>
<figure data-type="image" tabindex="30"><img src="https://Sunxiaonan0672.github.io/post-images/1568904617117.png" alt=""></figure>
<p>4.3 基于Negative Sampling 的模型</p>
<p>Negative Sampling  是Tomas Mikolov 等人在2013年在“Distributed Representation of words and Phrases and their Compositionality”一文中提出，主要目的是提高训练速度并改善所得词向量的质量，与Hierarchical Softmax 相比，不再使用哈夫曼树，而是利用随机的负采样技术。</p>
<p>5 小编寄语</p>
<p>作为处女座的小编，总是希望始末完整，因此本节主要介绍word2vec一部分，由于小编没有写完Negative Sampling 的模型，下次继续更新，然后借鉴小组大佬们的笔记，陆续分享elmo，bert 以及xlnet💪</p>
<p><strong>引用</strong></p>
<p>1.<a href="https://www.cnblogs.com/peghoty/p/3857839.html">word2vec 数学原理</a></p>
<p>2.<a href="https://www.cnblogs.com/pinard/p/7243513.html">word2vec原理</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PaperRobot]]></title>
        <id>https://Sunxiaonan0672.github.io/post/paperrobot</id>
        <link href="https://Sunxiaonan0672.github.io/post/paperrobot">
        </link>
        <updated>2019-07-10T06:09:52.000Z</updated>
        <content type="html"><![CDATA[<p><strong>PaperRobot: Incremental Draft Generation of Scientific Ideas</strong><br>
<img src="https://Sunxiaonan0672.github.io/post-images/1562739057240.png" alt=""></p>
<ol>
<li>PaperRobot 自动撰写论文，即它从以前的论文中提取背景知识图谱，产生新的科学思想，最后写出论文的关键要素</li>
</ol>
<p>2.工作流程如下</p>
<p>1）对特定目标领域的大量人类撰写的论文进行深入的理解，并构建全面的背景知识图(knowledge graphs, KGs)</p>
<p>2）通过结合从图注意力(graph attention)和上下文文本注意力(contextual text attention)，从背景知识库KG中预测链接，从而产生新想法</p>
<p>3）基于memory-attention网络，逐步写出一篇新论文的一些关键要素：从输入标题和预测的相关实体，生成一篇摘要；从摘要生成结论和未来工作；最后从未来工作生成下一篇论文的标题</p>
<p>3.具体介绍</p>
<p>1）Approach</p>
<p><strong>Background Knowledge Extraction</strong></p>
<p>2012年美国科研家研究表明，人类的阅读能力几乎是不变的，平均每年的阅读264篇论文，然而论文以井喷式的速度增长，例如，在生物医学领域，平均每年有超过50w篇论文被发表，仅2016年就有超过120w篇论文被发表，总论文数超过2600w篇</p>
<p>PaperRobot自动阅读现有的论文以构建背景知识图（KGS），其中节点是实体/概念，边缘是这些实体之间的关系。</p>
<p>本文作者应用的是 Wei 等人在2013年提出的实体和关系提取系统，他们输入生物医学领域的已发表论文，PaperRobot从中提取出3类知识概念：疾病，化学和基因。然后进一步将所有知识概念类型链接到 CTD (比较遗传毒理学数据库)，提取出133个子类型的关系，比如标记/机制、治疗和提高表达。</p>
<p>** Link Prediction**</p>
<p>构建完成最初的知识图谱后，通过执行 “link prediction”丰富补充知识图谱，</p>
<p>文中介绍上下文文本信息和图形结构对于表示一个实体都很重要，因此我们将它们结合起来为每个实体生成一个丰富的表示。基于实体表示，我们计算是否任意两个实体相似，如果两个实体相似，we propagate the neighbors of one entity to the other.<br>
比如：Calcium and Zinc 在文本信息以及知识图谱结构信息上是相似的，predict two new neighbors for Calcium: CD14 molecule and neuropilin 2 which are neighbors of Zinc in the initial KGs<br>
如下图所示</p>
<p>初始知识图谱用tuple表示</p>
<figure data-type="image" tabindex="1"><img src="https://Sunxiaonan0672.github.io/post-images/1563500948021.png" alt=""></figure>
<p>e_i 相关的文本描述s_i, s_i  is randomly selected from the sentences where ei occurs.</p>
<p>randomly initialize vector representations e_i and r_i for e_i and r_i respectively</p>
<p><strong>1)Graph Structure Encoder</strong></p>
<p>为了获取每一个neighbor’s feature to e_i，利用self-attention and compute a weight distribution over N_e_i</p>
<figure data-type="image" tabindex="2"><img src="https://Sunxiaonan0672.github.io/post-images/1563503036019.png" alt=""></figure>
<p><strong>2）Contextual Text En- coder</strong></p>
<p>每个实体e_i 都与一个文本句子[w_1,...w_l]相关 ,为了结合上线文本信息，LSTM 被用来获取hidden states H_s = [h_1 , ..., h_l ], where h_i represents the hidden state of w_i,Then we com- pute a bilinear attention weight for each word w_i</p>
<figure data-type="image" tabindex="3"><img src="https://Sunxiaonan0672.github.io/post-images/1563503671486.png" alt=""></figure>
<p><strong>3) Gated Combination</strong></p>
<p>To combine the graph-based representation e ̃ and local context based representations eˆ, we design a gate function to balance these two types of information:</p>
<figure data-type="image" tabindex="4"><img src="https://Sunxiaonan0672.github.io/post-images/1563503812684.png" alt=""></figure>
<p><strong>4)Training and Prediction</strong><br>
假设  h + r 约等于 t if (h, r, t) holds</p>
<figure data-type="image" tabindex="5"><img src="https://Sunxiaonan0672.github.io/post-images/1563504123677.png" alt=""></figure>
<figure data-type="image" tabindex="6"><img src="https://Sunxiaonan0672.github.io/post-images/1563504357591.png" alt=""></figure>
<p><strong>New Paper Writing</strong></p>
<p>基于输入的标题和预测的相关实体，可以自动写出了一篇新论文的摘要，根据摘要还可以写出结论部分和展望部分，通过展望甚至还可以生成新的标题。</p>
<p>首先给出 a reference title T= [w1, ..., wl]，利用知识实体抽取方法抽取title中的实体，并且从知识图谱中检索出一系列相关实体，并根据confidence scores 筛选出前10个相关实体 E_t= [e_t1 , ..., e_tv ]. Then we feed T and E_t together into the paper generation framework as shown in Figure 2.</p>
<figure data-type="image" tabindex="7"><img src="https://Sunxiaonan0672.github.io/post-images/1563507717438.png" alt=""></figure>
<p><strong>1)Reference Encoder</strong><br>
we randomly embed it into a vector and obtain T = [w1 , ..., wl ]. Then, we apply a bi-directional Gated Recurrent Unit (GRU) encoder  on T to produce the en- coder hidden states H = [h1, ..., hl]</p>
<p><strong>2)Decoder Hidden State Initialization</strong><br>
筛选出的实体并非全都是相关的，因此利用了memory- attention networks to further filter the irrelevant ones.<br>
related entities E=[e1,...,ev], we randomly initialize their vector representation E = [e1,...,ev]<br>
use the last hidden state of reference en- coder hl as the first query vector q0, and iteratively compute the attention distribution over all memo- ries and update the query vector:</p>
<figure data-type="image" tabindex="8"><img src="https://Sunxiaonan0672.github.io/post-images/1563515108056.png" alt=""></figure>
<p><strong>3）Memory Network</strong><br>
To better capture the contri- bution of each entity ej to each decoding output, at each decoding step i, we compute an attention weight for each entity and apply a memory net- work to refine the weights multiple times. We take the hidden state h ̃ i as the initial query q ̃0 = h ̃ i and iteratively update it:</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1563515347292.png" alt=""><br>
<img src="https://Sunxiaonan0672.github.io/post-images/1563515426216.png" alt=""></p>
<p><strong>4）Reference Attention</strong></p>
<p>获取reference title中的每个word 对 decoding output 的奉献，At each time step i, the decoder receives the pre- vious word embedding and generate decoder state h ̃ i , the attention weight of each reference token is computed as:<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1563516000093.png" alt=""></p>
<p><strong>5）Generator</strong></p>
<p>at each decoding step i, for each word w, we aggregate its attention</p>
<figure data-type="image" tabindex="9"><img src="https://Sunxiaonan0672.github.io/post-images/1563517126945.png" alt=""></figure>
<figure data-type="image" tabindex="10"><img src="https://Sunxiaonan0672.github.io/post-images/1563517193213.png" alt=""></figure>
<p><strong>6）Repetition Removal</strong><br>
apply beam search（波速搜索） with beam size 4 to generate each output, if a word is not a stop word or punctuation and it is already generated in the previous context, we will not choose it again in the same output.</p>
<p>beam search（波速搜索）具体实现后续更新。。。</p>
<p><strong>Experiment</strong></p>
<p><strong>1）Data</strong></p>
<p>biomedical papers from the PMC Open Access Subset.</p>
<p>To construct ground truth for new title prediction, if a human written paper A cites a paper B, we assume the title of A is gen- erated from B’s conclusion and future work ses- sion.</p>
<p><strong>2）Automatic Evaluation</strong></p>
<p>后续更新。。。。</p>
<p><strong>Remaining Challenges</strong><br>
Our generation model is still largely dependent on language model and extracted facts, and thus it lacks of knowledge reasoning.</p>
<p><strong>总结</strong><br>
实验部分还没有看完，继续更新中。。。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommendation]]></title>
        <id>https://Sunxiaonan0672.github.io/post/recommendation</id>
        <link href="https://Sunxiaonan0672.github.io/post/recommendation">
        </link>
        <updated>2019-05-07T08:25:07.000Z</updated>
        <summary type="html"><![CDATA[<h4 id="explainable-reasoning-over-knowledge-graphs-for-recommendation">Explainable Reasoning over Knowledge Graphs for Recommendation</h4>
<figure data-type="image" tabindex="1"><img src="https://Sunxiaonan0672.github.io/post-images/1557218821493.png" alt=""></figure>
<p><strong>背景介绍</strong></p>
<p>近年来整合知识图谱到推进系统中受到高度关注，通过探索知识图中的链接，可以发现用户和项目之间的连接作为路径，为用户项目交互提供丰富和充分的信息。然而这种连接不仅揭示了实体与关系的语义，而且有助于推断出用户的兴趣。但是，现有的工作还没有充分探索这种连接来推断用户偏好，特别是在对路径内部的顺序依赖性和整体语义进行建模方面。</p>
<p><strong>本文目的</strong></p>
<p>本文设计了一个新的模型，Knowledge- aware Path Recurrent Network (KPRN) ，建模用户-物品对在知识图谱中存在的关联路径，为用户提供可解释的推荐。通过知识图谱，用户-物品间的交互可以通过知识图谱找到的关联路径作解释。这类关联路径不仅表述了知识图谱中实体和关系的语义，还能够帮助我们理解用户的兴趣偏好，赋予推荐系统推理能力和可解释性。</p>
<p><strong>本文模型优势</strong></p>
<p>本文模型加入了一个新的权重池化操作来区分不同路径“用户-商品”的优势，使模型具有一定的可解释性。</p>
]]></summary>
        <content type="html"><![CDATA[<h4 id="explainable-reasoning-over-knowledge-graphs-for-recommendation">Explainable Reasoning over Knowledge Graphs for Recommendation</h4>
<figure data-type="image" tabindex="1"><img src="https://Sunxiaonan0672.github.io/post-images/1557218821493.png" alt=""></figure>
<p><strong>背景介绍</strong></p>
<p>近年来整合知识图谱到推进系统中受到高度关注，通过探索知识图中的链接，可以发现用户和项目之间的连接作为路径，为用户项目交互提供丰富和充分的信息。然而这种连接不仅揭示了实体与关系的语义，而且有助于推断出用户的兴趣。但是，现有的工作还没有充分探索这种连接来推断用户偏好，特别是在对路径内部的顺序依赖性和整体语义进行建模方面。</p>
<p><strong>本文目的</strong></p>
<p>本文设计了一个新的模型，Knowledge- aware Path Recurrent Network (KPRN) ，建模用户-物品对在知识图谱中存在的关联路径，为用户提供可解释的推荐。通过知识图谱，用户-物品间的交互可以通过知识图谱找到的关联路径作解释。这类关联路径不仅表述了知识图谱中实体和关系的语义，还能够帮助我们理解用户的兴趣偏好，赋予推荐系统推理能力和可解释性。</p>
<p><strong>本文模型优势</strong></p>
<p>本文模型加入了一个新的权重池化操作来区分不同路径“用户-商品”的优势，使模型具有一定的可解释性。</p>
<!-- more -->
<h4 id="1-introduction"><strong>1. Introduction</strong></h4>
<p>前期实验表明整合辅助数据（user profiles &amp; item attributes）到推荐系统中，有利于提高推荐系统的准确性。由于知识图谱包含了广泛的辅助数据，比如商品项目的背景知识和它们之间的关系等，因此受到了高度关注。知识图谱items之间的关系表示为(Ed Sheeran, IsSingerOf, Shape of You)</p>
<p>Extra user-item connectivity information derived from KG endows recommender systems the ability of reasoning and explainability。Taking music recommendation as an example (Figure 1), a user is connected to I See Fire since she likes Shape of You sung by the same singer Ed Sheeran。</p>
<figure data-type="image" tabindex="2"><img src="https://Sunxiaonan0672.github.io/post-images/1557294044985.png" alt=""></figure>
<p><strong>Running Example:</strong> (Alice, Interact, Shape of You)^(Shape of You, SungBy, Ed Sheeran)^(Ed Sheeran, IsSingerOf, I See Fire) =&gt; (Alice, Interact, I See Fire).</p>
<p><strong>前期难点与本文创新</strong></p>
<p>Prior efforts on knowledge-aware recommendation are roughly categorized into <em>path and embedding fashion</em></p>
<ol>
<li>
<p>Path- based methods introduce meta-paths to refine the similarities between users and items，然而该方法不考虑用户-商品之间的关系，因此当元路径中出现相同的实体但不同关系时，很难确定路径的整体语义。</p>
</li>
<li>
<p>Knowledge graph embedding (KGE) techniques  ，利用该技术将items 进行规范表达，因此，items具有相似连接实体具有相似表达方式。尽管提高了用户意图推荐，但是缺乏推理能力。 Specially, it only considers direct relations between entities, rather than the multi-hop relation paths as the** Running Example shows.**</p>
</li>
<li>
<p>本文方法通过知识图谱构建用户-物品对间的关联，为用户行为提供了解释。KPRN模型通过LSTM学习关联路径的表示，考虑了实体、关系间产生的序列依赖性，具备一定的推理能力。</p>
</li>
</ol>
<h4 id="2-本文任务定义"><strong>2. 本文任务定义</strong></h4>
<p>KPRN not only generates representations for paths by accounting for both entities and relations, but also performs reasoning based on paths to infer user preference。</p>
<p><strong>knowledge Graph (KG)</strong></p>
<p>nodes are entities E and edges R denote their relations.<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1557298568504.png" alt=""></p>
<figure data-type="image" tabindex="3"><img src="https://Sunxiaonan0672.github.io/post-images/1557298629379.png" alt=""></figure>
<p>知识图谱中的三元组清晰的描述了items 之间的直接或者间接（多步）的关系特性，即组成了users与items之间的一条或多条路径。</p>
<figure data-type="image" tabindex="4"><img src="https://Sunxiaonan0672.github.io/post-images/1557299248167.png" alt=""></figure>
<figure data-type="image" tabindex="5"><img src="https://Sunxiaonan0672.github.io/post-images/1557299305569.png" alt=""></figure>
<figure data-type="image" tabindex="6"><img src="https://Sunxiaonan0672.github.io/post-images/1557299355135.png" alt=""></figure>
<p><strong>Task Definition</strong></p>
<p>给定用户u和目标物品i，以及 (u,i) 对间的知识图谱路径集合 P(u,i)={P_1, …, P_k} ，对用户u与物品i发生交互的可能性做估计（CTR）。 Y_{u,i} = f_theta(u,i|P(u,i)) 。最终根据 Y_{u,i} 排序做Top-N推荐。</p>
<figure data-type="image" tabindex="7"><img src="https://Sunxiaonan0672.github.io/post-images/1557299588690.png" alt=""></figure>
<h4 id="3-modeling">**3. Modeling **</h4>
<p><strong>流程:</strong></p>
<p>1.extract qualified paths between a user-item pair from the KG, each of which consists of the related entities and relation</p>
<ol start="2">
<li>
<p>then adopt a Long Short-Term Memory (LSTM) network to model the sequential dependencies of entities and relations.</p>
</li>
<li>
<p>a pooling operation is performed to aggregate the representations of paths to obtain prediction signal for the user-item pair.</p>
</li>
</ol>
<figure data-type="image" tabindex="8"><img src="https://Sunxiaonan0672.github.io/post-images/1557299932203.png" alt=""></figure>
<p><strong>输入：</strong> a set of paths of each user-item pair</p>
<ol>
<li>
<p>embedding layer to project three types of IDs information</p>
</li>
<li>
<p>LSTM layer 按顺序对元素进行编码，以捕获基于关系的实体的组合语义</p>
</li>
<li>
<p>pooling 层 用于组合多条路径并输出最终得分of the given user interacting the target item</p>
</li>
</ol>
<p><strong>embedding layer</strong></p>
<p>给出一个路径p_k， the type (e.g., person or movie) and specific value (e.g., Peter Jackson or The Hobbit II) of each entity into two separate embedding vectors<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1557303185969.png" alt=""></p>
<figure data-type="image" tabindex="9"><img src="https://Sunxiaonan0672.github.io/post-images/1557303251185.png" alt=""></figure>
<p><strong>LSTM layer</strong></p>
<figure data-type="image" tabindex="10"><img src="https://Sunxiaonan0672.github.io/post-images/1557303425258.png" alt=""></figure>
<p>h_(l-1) and x_(l-1) 被用来计算下一个 path-step l</p>
<figure data-type="image" tabindex="11"><img src="https://Sunxiaonan0672.github.io/post-images/1557303534321.png" alt=""></figure>
<p>Taking advantages of the memory state, the last state hL is capable of representing the whole path pk.<br>
通过建立p_k表达，我们目的预测 t = (u,interact,i) 的合理性。<br>
Towards this end, two fully-connected layers are adopted to project the final state into the predictive score for output, given by：<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1557304008775.png" alt=""></p>
<p><strong>pooling 层</strong></p>
<p>S = {s1,s2,···,sK} be the predictive scores for K paths, P(u, i) = {p_1, p_2, · · · , p_K }, connecting a user-item pair (u,i), where each element is calculated based on Equation (4). 最终的预测 the average of the scores of all paths, which is formulated as</p>
<figure data-type="image" tabindex="12"><img src="https://Sunxiaonan0672.github.io/post-images/1557304578195.png" alt=""></figure>
<p>然而 由于不同路径对模型具有不同的贡献，while Equation (5) fails to specify importance of each path。</p>
<figure data-type="image" tabindex="13"><img src="https://Sunxiaonan0672.github.io/post-images/1557304821664.png" alt=""></figure>
<h4 id="learning"><strong>Learning</strong></h4>
<p>我们可以把推荐问题看作二分类问题， observed user- item interaction is assigned a target value 1, otherwise 0<br>
采用negative log-likelihood 做为 objective function</p>
<figure data-type="image" tabindex="14"><img src="https://Sunxiaonan0672.github.io/post-images/1557305614281.png" alt=""></figure>
<p>并且conduct L2 regularization on the trainable parameters , 从而以防过拟合。</p>
<h4 id="4-experiments"><strong>4. Experiments</strong></h4>
<p><strong>Dataset Description</strong></p>
<p>应用于movie recommendation and music recommendation</p>
<p>其中，电影领域数据为 MovieLens-1M1 and IMDb2 datasets，named MI<br>
MovieLens-1M 提供 user-item interaction 数据,  IMDb 做为 KG 包含了有关电影的辅助信息，such as genre, actor, director, and writer等。</p>
<p>关于音乐领域，采用的数据来源KKBox，其中不仅包含了user-item interaction data，还包含了音乐的描述，如singer, songwriter, and genre 等。</p>
<p>如果user 评价movie，便设置user-movie pair 为positive with the target value of 1, and 0 otherwise。</p>
<p>user-songs 同理。</p>
<p>在训练过程中，For each positive user-item interaction pair in the training set, we conducted the negative sampling strategy to pair it with four negative items that the user has not interacted with.</p>
<p>在测试过程中，the ratio between positive and negative interactions is set as 1 : 100, namely, 100 negative items are randomly sampled and pair with one positive item in the testing set.</p>
<p><strong>Path Extraction</strong></p>
<p>we extract all qualified paths, each with length up to six, that connect all user-item pairs.</p>
<p><strong>Experimental Settings</strong></p>
<p><em>1. Evaluation Metrics</em></p>
<ol>
<li>
<p><strong>hit@K</strong> considers whether the relevant items are retrieved within the top K positions of the recommendation list.</p>
</li>
<li>
<p><strong>ndcg@K</strong> measures the relative orders among positive and negative items within the top K of the ranking list.</p>
</li>
</ol>
<p>We report the average metrics at K = {1,2,··· ,15} of all instances in the test set.</p>
<p><em>2. Baselines</em></p>
<p>**MF **   基于贝叶斯个性化排序（BPR）损失的矩阵分解方法，单独利用user-item</p>
<p><strong>NFM</strong> 该方法是一种以历史items 作为用户特征的最先进的因子分解模型</p>
<p><strong>CKE</strong>  embedding-based method，  which integrates the representations from Matrix Factorization  and TransR to enhance the recommendation</p>
<p><strong>FMG</strong>   This is a state-of-the-art meta-path based method，which predefines various types of meta-graphs and employs Matrix Factorization on each meta-graph similarity matrix to make recommendation</p>
<p><em>3 实验对比</em></p>
<p><strong>3.1 Performance Comparison</strong></p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1557369721211.png" alt="">****</p>
<p><strong>3.2. Study of KPRN</strong></p>
<p>本实验研究 路径中的关系和weighted pooling operation 对模型性能的影响</p>
<ol>
<li>Effects of Relation Modeling （without the relation modeling, termed as KPRN-r）</li>
</ol>
<figure data-type="image" tabindex="15"><img src="https://Sunxiaonan0672.github.io/post-images/1557370001845.png" alt=""></figure>
<p>2.Effects of Weighted Pooling （set the value   as {0.01,0.1,1,10}）</p>
<figure data-type="image" tabindex="16"><img src="https://Sunxiaonan0672.github.io/post-images/1557370149486.png" alt=""></figure>
<p><strong>When   decrease from 1 to 0.1, the weighted pooling operation degrades the performance, since it is similar to max-pooling and selects only the most important paths as the user-item connectivity.</strong></p>
<p><strong>The performance w.r.t. hit@K and ndcg@K becomes poorer, when increasing   from 1 to 10. It makes sense since it tends to aggregate contributions from more paths, rather than the most informative ones</strong></p>
<p><strong>3.3. Case Studies</strong><br>
随机选择一名用户（ID 为 u4825），从她的相互记录中选择<em>Shakespeare in Love</em></p>
<figure data-type="image" tabindex="17"><img src="https://Sunxiaonan0672.github.io/post-images/1557371826564.png" alt=""></figure>
<h4 id="总结">总结</h4>
<p>本文主要介绍基于知识图谱中信息做推荐，主要应用于电影和音乐领域。</p>
<p>----------------------------------------&gt; 来自一只奋力登高望远的IT🦐</p>
<h4 id="引用">引用</h4>
<ol>
<li>
<p><a href="https://arxiv.org/pdf/1811.04540.pdf">Explainable Reasoning over Knowledge Graphs for Recommendation</a>.AAAI 2019</p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/51000072">基于知识图谱路径推理的可解释推荐</a></p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[基于门控的RNN]]></title>
        <id>https://Sunxiaonan0672.github.io/post/RNN-based-on-gates</id>
        <link href="https://Sunxiaonan0672.github.io/post/RNN-based-on-gates">
        </link>
        <updated>2019-05-05T07:20:03.000Z</updated>
        <summary type="html"><![CDATA[<p>为了解决上一章节提到的<em>记忆容量</em>问题，<em>门控</em>被引入来控制信息的积累的速度，即包括有选择的加入新的信息，并且有选择的遗忘之前累计的信息。这一网络被称为<strong>基于门控的循环神经网络(Gated RNN )</strong></p>
<p>本章主要分享两种基于门控的循环神经网络：<em>长短期记忆网络(LSTM)</em>  和<em>门控循环单元(GRU)</em></p>
]]></summary>
        <content type="html"><![CDATA[<p>为了解决上一章节提到的<em>记忆容量</em>问题，<em>门控</em>被引入来控制信息的积累的速度，即包括有选择的加入新的信息，并且有选择的遗忘之前累计的信息。这一网络被称为<strong>基于门控的循环神经网络(Gated RNN )</strong></p>
<p>本章主要分享两种基于门控的循环神经网络：<em>长短期记忆网络(LSTM)</em>  和<em>门控循环单元(GRU)</em></p>
<!-- more -->
<h2 id="1-lstm">1.  LSTM</h2>
<p>1 . LSTM  简介</p>
<p>LSTM 是循环神经网络的一个变体，可以有效的解决简单的梯度消失或消失问题。</p>
<figure data-type="image" tabindex="1"><img src="https://Sunxiaonan0672.github.io/post-images/1557049727798.png" alt=""></figure>
<p>公式(1)                                  h_t = h_(t−1) + g(x_t, h_(t−1); θ)</p>
<p>在公式(1)的基础上，LSTM 网络主要更改的为：</p>
<p><strong>1. 新的内部状态(internal states)</strong> c_t专门进行线性的循环信息传递，同时(非线性)输出信息给隐藏层的外部状态h_t。</p>
<figure data-type="image" tabindex="2"><img src="https://Sunxiaonan0672.github.io/post-images/1557043623136.png" alt=""></figure>
<figure data-type="image" tabindex="3"><img src="https://Sunxiaonan0672.github.io/post-images/1557044000039.png" alt=""></figure>
<p>其中 f_t，i_t 和 o_t 为三个门(gate)来控制信息传递的路径;⊙ 为向量元素乘积; c_(t−1) 为上一时刻的记忆单元;c ̃t是通过非线性函数得到候选状态。</p>
<p>在每个时刻 t，LSTM 网络的内部状态 c_t 记录了到当前时刻为止的历史信息。</p>
<p><strong>2. 机制门</strong><br>
LSTM 引入“门机制”来控制信息传递路径。LSTM网络中的“门”是一种“软门”，取值{0,1}之间，表示一定比例运行信息进入。</p>
<figure data-type="image" tabindex="4"><img src="https://Sunxiaonan0672.github.io/post-images/1557045809930.png" alt=""></figure>
<p>其中 σ(·) 为 logistic 函数，其输出区间为 (0, 1)，x_t 为当前时刻的输入，h_(t−1) 为 上一时刻的外部状态</p>
<p>其中</p>
<ol>
<li>
<p>遗忘门f_t控制上一个时刻的内部状态c_(t−1)需要遗忘多少信息</p>
</li>
<li>
<p>输入门i_t控制当前时刻的候选状态c ̃t有多少信息需要保存</p>
</li>
<li>
<p>输出门o_t控制当前时刻的内部状态c_t有多少信息需要输出给外部状态h_t</p>
</li>
</ol>
<p><strong>LSTM 计算过程</strong></p>
<figure data-type="image" tabindex="5"><img src="https://Sunxiaonan0672.github.io/post-images/1557046704003.png" alt=""></figure>
<p>LSTM循环单元结构</p>
<p>由上图所示</p>
<ol>
<li>
<p>首先利用上一时刻的外部状态h_(t-1)和当前时刻的输入x_t，计算三个门，以及候选状态c ̃t。</p>
</li>
<li>
<p>结合遗忘门f_t 和输入门 i_t 来更新记忆单元 c_t</p>
</li>
<li>
<p>结合输出门o_t，将 内部状态的信息传递给外部状态h_t。</p>
</li>
</ol>
<p><strong>记忆</strong></p>
<p>循环神经网络中的隐状态h存储了历史信息，可以看作是一种记忆(memory)。</p>
<p>在简单循环网络中，隐状态每个时刻都会被重写，因此可以看作是一种<em>短期记忆(short-term memory)</em>，在神经网络中，*长期记忆(long-term memory)*可以看作是网络参数，隐含了从训练数据中学到的经验，并更新周期要远远慢 于短期记忆。</p>
<p>在LSTM 网络中，记忆单元c 可以在某个时刻捕捉到某个关键性信息，并有能力将该信息保存一定的时间间隔。记忆单元c中保存信息的生命周期要长于短期记忆h，但又远远短于长期记忆，因此成为<strong>长的短期记忆(long short-term memory)</strong>。</p>
<h5 id="总结">总结</h5>
<p>本文分享了LSTM的基本结构，关于LSTM 的参数计算，会在以后章节中分享😊</p>
<p>--------------------------------------------------&gt;来自一只💪工作，心存理想的IT🦐的分享</p>
<h5 id="引用">引用</h5>
<ol>
<li>深度学习-邱锡鹏</li>
</ol>
<p>2 <a href="https://blog.csdn.net/u010900574/article/details/51823962">.理解长短期记忆(LSTM) 神经网络</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[解决方法-梯度爆炸或梯度消失]]></title>
        <id>https://Sunxiaonan0672.github.io/post/Methods</id>
        <link href="https://Sunxiaonan0672.github.io/post/Methods">
        </link>
        <updated>2019-05-05T03:02:38.000Z</updated>
        <summary type="html"><![CDATA[<p>为了避免梯度爆炸或者消失问题，最直接的方式就是选取合适的参数，同时使用非饱和的激活函数，尽量使得diag(f′(z_i)U_T ≈ 1，这种方式需要足够的 人工调参经验，限制了模型的广泛应用。</p>
<p>因此通过改进模型或者优化方法来缓解网络梯度爆炸或者消失</p>
]]></summary>
        <content type="html"><![CDATA[<p>为了避免梯度爆炸或者消失问题，最直接的方式就是选取合适的参数，同时使用非饱和的激活函数，尽量使得diag(f′(z_i)U_T ≈ 1，这种方式需要足够的 人工调参经验，限制了模型的广泛应用。</p>
<p>因此通过改进模型或者优化方法来缓解网络梯度爆炸或者消失</p>
<!-- more -->
<p><strong>1. 梯度爆炸：</strong></p>
<ul>
<li>梯度爆炸* 通过权重衰减或梯度截断来避免</li>
</ul>
<p>权重衰减是通过给参数增加 L_1 或 L_2 范数的正则化项来限制参数的取值范 围，从而使得 γ ≤ 1。</p>
<p>梯度截断是另一种有效的启发式方法，当梯度的模大于一 定阈值时，就将它截断成为一个较小的数。</p>
<p><strong>2. 梯度消失：</strong><br>
<em>梯度消失</em> 是循环网络的主要问题，除了通过优化技巧来避免梯度消失问题，改进模型是更有效的方法</p>
<p>比如：<em>U = I</em> ， 同时* f′(z_i) = 1*</p>
<p><em>h_t =h_(t−1 )+g(x_t;θ)</em>                               其中 g(·) 是一个非线性函数，θ 为参数</p>
<p>由以上公式可知，h_t 和h_(t-1)之间为线性依赖关系，且权重系数为1，这样就不存在梯度消失问题了。</p>
<p>然而，这种变换也丢失了神经元在反馈边上的非线性激活的性质，从而降低了模型的表达能力。、</p>
<p>改进：<br>
<em>h_t = h_(t−1) + g(x_t, h_(t−1); θ)</em><br>
这样h_t和h_(t-1)既有线性关系也有非线性关系，这在一定程度上可以缓解梯度消失问题。但是随着h_t 不断积累存储新的输入信息，会产生饱和现象。</p>
<p>假设g(·)为logistic函数，则随着时间 t 的增长，h_t 会变得越来越大，从而导致 h 变得饱和。也就是说， 隐状态 h_t 可以存储的信息是有限的，随着记忆单元存储的内容越来越多，其丢失的信息也越来越多。这个问题就是就是<em>记忆容量(memory capacity)</em></p>
<h5 id="解决记忆容量方法">解决记忆容量方法</h5>
<ol>
<li><em>增加一些额外的存储单元:外部记忆</em></li>
<li><em>进行选择性的遗忘，同时也进行有选择的更新</em></li>
</ol>
<h5 id="总结">总结</h5>
<p>本章主要介绍了<em>梯度爆炸</em>和<em>梯度消失</em>的改进方案，然而导致<em>记忆容量</em>问题。关于<em>记忆容量</em>的解决方式下一章节会继续更新。</p>
<p>----------------------------------------------来自一只茁壮成长的IT小🦐的分享</p>
<h5 id="引用">引用</h5>
<ol>
<li>深度学习-邱锡鹏</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[长期依赖问题]]></title>
        <id>https://Sunxiaonan0672.github.io/post/chang-qi-yi-lai-wen-ti</id>
        <link href="https://Sunxiaonan0672.github.io/post/chang-qi-yi-lai-wen-ti">
        </link>
        <updated>2019-04-28T06:36:34.000Z</updated>
        <summary type="html"><![CDATA[<h4 id="长期依赖问题的出现">长期依赖问题的出现</h4>
<p>在循环神经网络（RNN）中存的一个问题便是<em>长期依赖文同</em></p>
]]></summary>
        <content type="html"><![CDATA[<h4 id="长期依赖问题的出现">长期依赖问题的出现</h4>
<p>在循环神经网络（RNN）中存的一个问题便是<em>长期依赖文同</em></p>
<!-- more -->
<p>根据上一章节中介绍的BPTT算法，其中第t时刻损失函数对第k时刻隐藏层的净输入z_k的导数如下：</p>
<figure data-type="image" tabindex="1"><img src="https://Sunxiaonan0672.github.io/post-images/1556439290762.png" alt=""></figure>
<p>将以上公式展开为：</p>
<figure data-type="image" tabindex="2"><img src="https://Sunxiaonan0672.github.io/post-images/1556439446414.png" alt=""></figure>
<p>以上展开式，我们可以发现～</p>
<p>梯度爆炸问题：如果γ &gt; 1，当t−k → ∞时，γ(t−k) → ∞，会造成系统不稳定,则称作梯度爆炸<br>
梯度消失问题：如果γ &lt; 1，当t−k → ∞时，γ(t−k) → 0， 会出现和深度前馈神经网络类似的梯度消失问题</p>
<p>由于RNN经常实用非线性激活函数为logistic 函数以及tanh函数，其导数图示如下：</p>
<ol>
<li>logistic 函数（sigmoid 函数）</li>
</ol>
<figure data-type="image" tabindex="3"><img src="https://Sunxiaonan0672.github.io/post-images/1556440773011.png" alt=""></figure>
<p>2.tanh 函数</p>
<figure data-type="image" tabindex="4"><img src="https://Sunxiaonan0672.github.io/post-images/1556440926561.jpg" alt=""></figure>
<p>从上图可以发现 sigmoid 函数的值域为（0，0.25]，tanh的值域也小于1，则如果时间间隔 t-k 过大，，δ_(t,k) 会趋向于 0，因此经常会出现梯度消失问题。</p>
<p>因此，理论上简单的循环神经网络可以长时间间隔状态之间的依赖关系，但是由于梯度爆炸或者消失问题，实际上只能学习到短期的依赖关系。</p>
<p>如果 t 时刻的输出 yt 依赖于 t − k 时刻的输入 x_(t−k) ，当间隔 k 比较大时，简单神经网络很难建模这种长距离的依赖关系，称为长期依赖问题(Long-Term Dependencies Problem)。</p>
<p><strong>注意⚠️：</strong>  在循环神经网络中的梯度消失不是说 ∂L_t / ∂U 的梯度消失了， 而是∂L_t  / ∂h_k 的梯度消失了(当t−k比较大时)</p>
<h4 id="总结">总结</h4>
<p>本章主要分享了在实际中循环神经网络中容易出现依赖问题，下一章则会介绍解决方法。</p>
<p>如有错误，请各路大神留言指教～🙏 🦐不胜感激。</p>
<p>--------------------------------------------------------来自一枚努力学习工作的IT🦐</p>
<h4 id="引用">引用</h4>
<ol>
<li>
<p>深度学习-邱锡鹏</p>
</li>
<li>
<p>详解机器学习中的梯度消失、爆炸原因及其解决方法 <a href="https://blog.csdn.net/qq_25737169/article/details/78847691"></a></p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[循环神经网络-RNN之参数学习]]></title>
        <id>https://Sunxiaonan0672.github.io/post/RNN-参数学习</id>
        <link href="https://Sunxiaonan0672.github.io/post/RNN-参数学习">
        </link>
        <updated>2019-04-26T02:11:56.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>RNN参数学习</strong>  — 上一小结简单讲述了RNN的基本结构和应用模式。本章主要分享一下该网络的参数学习。</p>
]]></summary>
        <content type="html"><![CDATA[<p><strong>RNN参数学习</strong>  — 上一小结简单讲述了RNN的基本结构和应用模式。本章主要分享一下该网络的参数学习。</p>
<!-- more -->
<p>循环神经网络的参数可以通过梯度下降方法来进行学习。</p>
<p>在循环神经网络中主要有两种计算梯度的方式：随时间<em>反向传播（BPTT）</em>  和 <em>实时循环学习（RTRL）</em></p>
<p>接下来主要介绍这两种计算梯度的方式</p>
<!-- more -->
<h4 id="损失函数">损失函数</h4>
<p>本文均以随机梯度下降为🌰，损失函数可以采用交叉熵。损失函数的具体详情如下</p>
<figure data-type="image" tabindex="1"><img src="https://Sunxiaonan0672.github.io/post-images/1556250122937.jpeg" alt=""></figure>
<p>因此，整个序列的损失函数为</p>
<figure data-type="image" tabindex="2"><img src="https://Sunxiaonan0672.github.io/post-images/1556250295330.png" alt=""></figure>
<h4 id="参数计算">参数计算</h4>
<p>以整个序列损失函数 L 关于参数 U 的梯度为例🌰，其梯度为</p>
<figure data-type="image" tabindex="3"><img src="https://Sunxiaonan0672.github.io/post-images/1556250622728.png" alt=""></figure>
<p>即每个时刻损失 Lt 对参数 U 的偏导数之和。</p>
<h4 id="梯度计算方式">梯度计算方式</h4>
<ol>
<li>
<p><strong>随时间反向传播算法（BPTT）</strong></p>
<p>BPTT 算法将循环神经网络看作是一个展开的多层前馈网络，其中“每一 层”对应循环网络中的“每个时刻”（如下图所示）<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1556251001069.jpg" alt=""></p>
<p>在“展开”的循环神经网络中，所有层的参数是共享的（因此参数的真实梯度是将所有“展开层”的参数梯度 之和）<br>
<strong>计算偏导数</strong></p>
<p>🌰 ～ 根据RNN的损失函数，先计算第 t 时刻损失对参数 U 的偏导数 ∂L_t／∂U</p>
<p>因为参数U 跟隐藏层每个时刻k（1&lt;= k &lt;=t）的净输入z_k = U h_k−1 + W x_k +b 有关，因此<br>
因此第t时刻损失的损失函数L_t关于参数U的梯度为下图<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1556421323127.jpeg" alt=""></p>
<p>随时间反向传播算法示例<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1556421386145.png" alt=""></p>
</li>
<li>
<p><strong>实时循环学习（RTRL）</strong><br>
与反向传播算法不同的是，实时传播算法（Real-Time Recurrent Learning，RTRL）是通过前向传播的方式来计算梯度。<br>
具体操作如下图所示<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1556430047120.jpeg" alt=""></p>
</li>
</ol>
<p>参数 W 和 b 的梯度也可以同样按上述方法实时计算。</p>
<h4 id="bptt与rtrl比较">BPTT与RTRL比较</h4>
<p>前者通过反向模式计算梯度，后者利用前向模式计算梯度。 BPTT 算法的计算量会更小，但是 BPTT 算法需 要保存所有时刻的中间梯度，空间复杂度较高。RTRL 算法不需要梯度回传，因 此非常适合用于需要在线学习或无限序列的任务中。</p>
<h4 id="总结">总结</h4>
<p>这一章节总结了循环神经网络的梯度计算，奈何本人学识有限，存在错误的地方请多多指教🙏</p>
<p>-----------------------------------------------------------来自一枚努力学习工作的IT🦐</p>
<h5 id="引用">引用</h5>
<ol>
<li>深度学习-邱锡鹏</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[循环神经网络-RNN之简介]]></title>
        <id>https://Sunxiaonan0672.github.io/post/RNN-Introduction</id>
        <link href="https://Sunxiaonan0672.github.io/post/RNN-Introduction">
        </link>
        <updated>2019-04-25T08:35:40.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>循环神经网络简介</strong><br>
由于前馈神经网络的输入只依赖当前输入，难以处理时序数据，比如视频，语音，文本等。时序数据的不固定性也使得前馈神经网络无法有效的处理该问题。<br>
循环神经网络（Recurrent Neural Network）是一款具有短期记忆功能的网络<br>
循环神经网络通过自身反馈神经元，可以处理任意长度的时序数据。</p>
]]></summary>
        <content type="html"><![CDATA[<p><strong>循环神经网络简介</strong><br>
由于前馈神经网络的输入只依赖当前输入，难以处理时序数据，比如视频，语音，文本等。时序数据的不固定性也使得前馈神经网络无法有效的处理该问题。<br>
循环神经网络（Recurrent Neural Network）是一款具有短期记忆功能的网络<br>
循环神经网络通过自身反馈神经元，可以处理任意长度的时序数据。</p>
<!-- more -->
<p>假设 在时刻t时，网络输入为x_t，给定一个序列x_1:T=（x_1，x_2，x_3，...x_T) ，循环神经网络通过公式（1）来更新带反馈边的隐藏层的活性值h_t</p>
<pre><code>                               h_t = f(h_（t-1）,x_t)                   (1)
</code></pre>
<p>结构下图所示<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1556183190436.png" alt=""><br>
图 1 循环神经网络</p>
<p>假设在时刻t时，网络的输入为x_t，隐藏层状态(即隐藏层神经元活性值) 为 h_t 不仅和当前时刻的输入 x_t 相关，也和上一个时刻的隐藏层状态 h_t−1 相关。</p>
<pre><code>			z_t =Uh_(t−1) +Wx_t +b			(2)
			ht = f(zt)                              (3)
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://Sunxiaonan0672.github.io/post-images/1557049592841.png" alt=""></figure>
<h4 id="循环神经网络的应用模式">循环神经网络的应用模式</h4>
<ol>
<li>
<p>序列到类别模式<br>
序列类别模式主要应用于序列数据分类问题，输入---&gt;序列，输出---&gt;类别<br>
例如输入<em>文本单词序列</em>，输出<em>文本类别</em><br>
网络结构如下所示<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1556189860031.png" alt=""></p>
<p>图 2 序列到类别模式</p>
<p>图2 （a）将h_T 看作整个序列的最终特征并输入给分类器 g(·) 进行分类，如公式4所示</p>
<pre><code>yˆ = g(h_T )                     （4）
</code></pre>
</li>
</ol>
<p>图2 （b）对整个序列的所有 状态进行平均，并用这个平均状态来作为整个序列的表示，公式如下图所示</p>
<figure data-type="image" tabindex="2"><img src="https://Sunxiaonan0672.github.io/post-images/1556191189374.png" alt=""></figure>
<ol start="2">
<li>同步的序列到序列模式</li>
</ol>
<p>同步的序列到序列模式主要用于序列标注(Sequence Labeling)任务，即每 一时刻都有输入和输出，输入序列和输出序列的长度相同，<em>词性标注</em>就是典型的同步序列到序列模式。<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1556191431221.png" alt=""></p>
<p>图3 同步序列模式</p>
<ol start="3">
<li>异步的序列到序列模式<br>
异步的序列到序列模式也称为编码器-解码器(Encoder-Decoder)模型，即输入序列和输出序列不需要有严格的对应关系，也不需要保持相同的长度。主要应用于“机器翻译”。输入为一个长度为 T 的序列 x_1:T = (x_1,··· ,x_T)，输出为长度为M的序列y_1:M = (y_1,··· ,y_M)。<br>
利用先<em>编码</em>后<em>解码</em>的模式，先将x输入到一个循环神经网络（编码器）中获得h_T，再使用另一个循环神经网络（解码器）得到输出序列y_1:M<br>
<img src="https://Sunxiaonan0672.github.io/post-images/1556191993257.png" alt=""></li>
</ol>
<p>图 4 异步序列模式</p>
<p>其中f1(·), f2(·)分别为用作编码器和解码器的循环神经网络，g(·)为分类器，yˆt为预测输出 yˆt 的向量表示，公式如下</p>
<p><img src="https://Sunxiaonan0672.github.io/post-images/1556192403766.png" alt=""><br>
<img src="https://Sunxiaonan0672.github.io/post-images/1556192412309.png" alt=""></p>
<h4 id="作者总结">作者总结</h4>
<p>基于时间问题，第一部分简介终结与此，第二部分会介绍RNN的参数学习</p>
<p>----------------------------来自一枚努力工作的IT🦐</p>
<h5 id="citing~">citing~</h5>
<ol>
<li>深度学习----邱锡鹏</li>
</ol>
]]></content>
    </entry>
</feed>